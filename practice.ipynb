{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe81c1897b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab;tagger=Mecab()\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import numpy as np\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w] if w in to_ix.keys() else to_ix[\"<UNK>\"], seq))\n",
    "    tensor = Variable(torch.LongTensor(idxs)).cuda() if USE_CUDA else Variable(torch.LongTensor(idxs))\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 트레이닝 : 286,817\n",
    "* 벨리데이션 : 13,368\n",
    "* 테스트 : 11,487\n",
    "* 인풋 : 800토큰, 아웃풋 100 토큰\n",
    "* 인풋 : 헤드라인, 바이라인(필자 등), 본문을 스페셜 토큰으로 조인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = open('insight_social.txt','r',encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=[]\n",
    "temp=[]\n",
    "for r in raw:\n",
    "    if r !='\\n':\n",
    "        temp.append(r[:-1])\n",
    "    else:\n",
    "        data.append(temp)\n",
    "        temp=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, d in enumerate(data):\n",
    "    try:\n",
    "        temp = []\n",
    "        temp.append(d[0].split(\"제목 : \")[1])\n",
    "        temp.append(d[2].split(\"본문 : \")[1].replace(\"\\'\",\"\"))\n",
    "        input_seq = \"ssSEPARATEss\".join(temp)\n",
    "        train_set.append((input_seq, d[1].split(\"요약 : \")[1]))\n",
    "    except:\n",
    "        pass\n",
    "        #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = [[tagger.morphs(t[0]),tagger.morphs(t[1])] for t in train_set if len(tagger.morphs(t[0]))<500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = zip(*train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index ={\"<PAD>\":0,\"<START>\":1,\"<END>\":2,\"<UNK>\":3}\n",
    "\n",
    "for token in flatten(X)+flatten(y): # flatten(y)는 없는 취급하고 나중에 OOV 때 copy 일어나도록\n",
    "    if token not in word2index.keys():\n",
    "        word2index[token]=len(word2index)\n",
    "\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18143"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_p=[]\n",
    "y_p=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "\n",
    "    if len(X[i])<300:\n",
    "        num_pad = 300-len(X[i])\n",
    "        temp_x = X[i]+[\"<PAD>\"]*num_pad\n",
    "    else:\n",
    "        temp_x  = X[i][:300]\n",
    "    X_p.append(prepare_sequence(temp_x,word2index))\n",
    "\n",
    "    if len(y[i])<50:\n",
    "        num_pad = 50-len(y[i])\n",
    "        temp_y = y[i] +[\"<PAD>\"]*num_pad\n",
    "    else:\n",
    "        temp_y = y[i][:50]\n",
    "    y_p.append(prepare_sequence(temp_y,word2index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = list(zip(X_p,y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(train_data,batch_size):\n",
    "    batch = random.sample(train_data,batch_size)\n",
    "    \n",
    "    X,y = zip(*batch)\n",
    "    \n",
    "    X = torch.cat(list(X)).view(batch_size,-1)\n",
    "    y = torch.cat(list(y)).view(batch_size,-1)\n",
    "    mask = torch.cat([Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, t.data)))).cuda() if USE_CUDA else Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, t.data)))) for t in X]).view(BATCH_SIZE,-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return X,mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size,embedding_size, hidden_size,batch_size=32 ,n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True,bidirectional=True)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.randn(self.n_layers*2, self.batch_size, self.hidden_size)).cuda() if USE_CUDA else Variable(torch.randn(self.n_layers*2, self.batch_size, self.hidden_size))\n",
    "        context = Variable(torch.randn(self.n_layers*2, self.batch_size, self.hidden_size)).cuda() if USE_CUDA else Variable(torch.randn(self.n_layers*2, self.batch_size, self.hidden_size))\n",
    "        return (hidden,context)\n",
    "    \n",
    "        \n",
    "    def forward(self, input,input_masking):\n",
    "        \"\"\"\n",
    "        input : B,T (LongTensor)\n",
    "        input_masking : B,T (PAD 마스킹한 ByteTensor)\n",
    "        \n",
    "        <PAD> 제외한 리얼 Context를 다시 만들어서 아웃풋으로\n",
    "        \"\"\"\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        output, self.hidden = self.lstm(embedded, self.hidden)\n",
    "        \n",
    "        real_context=[]\n",
    "        \n",
    "        for i,o in enumerate(output): # B,T,D\n",
    "            real_length = input_masking[i].data.tolist().count(0) # 실제 길이\n",
    "            real_context.append(o[real_length-1])\n",
    "            \n",
    "        return output, torch.cat(real_context).view(self.batch_size,-1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,output_size,embedding_size,hidden_size,max_len=50,batch_size=32,n_layers=1,dropout_p=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_len=max_len\n",
    "        self.embedding_size = embedding_size\n",
    "        self.batch_size = batch_size\n",
    "        self.attn_e = nn.Linear(self.hidden_size,self.hidden_size) # encoder Attention\n",
    "        self.attn_d = nn.Linear(self.hidden_size,self.hidden_size) # encoder Attention\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.temp_attn_scores=[]\n",
    "\n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(self.output_size, self.embedding_size) #TODO encoder와 공유하도록 하고 학습되지 않게..\n",
    "\n",
    "        #self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(self.embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "        \n",
    "        self.switch = nn.Linear(self.hidden_size*3, 1) # Gen or Copy\n",
    "        self.out = nn.Linear(self.hidden_size*3, self.output_size)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.randn(self.n_layers*2, self.batch_size, self.hidden_size)).cuda() if USE_CUDA else Variable(torch.randn(self.n_layers*2, self.batch_size, self.hidden_size))\n",
    "        context = Variable(torch.randn(self.n_layers*2, self.batch_size, self.hidden_size)).cuda() if USE_CUDA else Variable(torch.randn(self.n_layers*2, self.batch_size, self.hidden_size))\n",
    "        return (hidden,context)\n",
    "    \n",
    "    \n",
    "    def IntraEncAttention(self, hidden, encoder_outputs, encoder_masking):\n",
    "        \"\"\"\n",
    "        hidden : 1,B,D\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_masking : B,T # ByteTensor\n",
    "        \"\"\"\n",
    "        hidden = hidden.squeeze(0).unsqueeze(2)  # 히든 : (1,배치,차원) -> (배치,차원,1)\n",
    "        \n",
    "        batch_size = encoder_outputs.size(0) # B\n",
    "        encoder_len = encoder_outputs.size(1) # T : 300\n",
    "        \n",
    "        e_tis=[]\n",
    "        for i, e_i in enumerate(encoder_outputs.transpose(0,1)):\n",
    "            e_i = self.attn_e(e_i) # BxD\n",
    "            \n",
    "            ## PAD MASKING\n",
    "            e_masking = encoder_masking.transpose(0,1)[i].unsqueeze(1).expand_as(e_i) # BxD\n",
    "            e_i.masked_fill(e_masking,-1e12) # masking\n",
    "            \n",
    "            temporal_score = e_i.unsqueeze(1).bmm(hidden).squeeze(1).exp() # Bx1\n",
    "            self.temp_attn_scores.append(temporal_score)\n",
    "            \n",
    "            if len(self.temp_attn_scores)>1:\n",
    "                temporal_score = temporal_score/torch.sum(torch.cat(self.temp_attn_scores[:-1],1),1)\n",
    "            \n",
    "            e_tis.append(temporal_score)\n",
    "        \n",
    "        contexts=[]\n",
    "        normalized_attn_scores=[]\n",
    "        for i,e_i in enumerate(encoder_outputs.transpose(0,1)):\n",
    "            normalized_attn_score = e_tis[i]/torch.sum(torch.cat(e_tis,1),1)\n",
    "            normalized_attn_scores.append(normalized_attn_score)\n",
    "            context = normalized_attn_score.unsqueeze(2).bmm(e_i.unsqueeze(1)) # Bx1x1 , Bx1xD = Bx1xD\n",
    "            contexts.append(context)\n",
    "        \n",
    "        _, copy_attn_index = torch.max(F.softmax(torch.cat(normalized_attn_scores,1)),1)\n",
    "        \n",
    "        enc_context = torch.sum(torch.cat(contexts,1),1)\n",
    "        \n",
    "        del e_tis\n",
    "        del contexts\n",
    "        del normalized_attn_scores\n",
    "        \n",
    "        return  enc_context, copy_attn_index # B,1,D\n",
    "    \n",
    "    def IntraDecAttention(self,hidden, decoder_outputs):\n",
    "        \"\"\"\n",
    "        hidden : 1,B,D\n",
    "        decoder_outputs : B,t,D\n",
    "        \"\"\"\n",
    "        hidden = hidden.squeeze(0).unsqueeze(2)  # 히든 : (1,배치,차원) -> (배치,차원,1)\n",
    "        \n",
    "        batch_size = decoder_outputs.size(0) # B\n",
    "        max_len = decoder_outputs.size(1) # t\n",
    "        \n",
    "        d_tis=[]\n",
    "        for d_i in decoder_outputs.transpose(0,1):\n",
    "            d_i = self.attn_d(d_i) # BxD\n",
    "            temporal_score = d_i.unsqueeze(1).bmm(hidden).squeeze(1).exp() # Bx1\n",
    "            d_tis.append(temporal_score)\n",
    "        \n",
    "        contexts=[]\n",
    "        for i,d_i in enumerate(decoder_outputs.transpose(0,1)):\n",
    "            normalized_attn_score = d_tis[i]/torch.sum(torch.cat(d_tis,1),1)\n",
    "            context = normalized_attn_score.unsqueeze(2).bmm(d_i.unsqueeze(1)) # Bx1x1 , Bx1xD = Bx1xD\n",
    "            contexts.append(context)\n",
    "        \n",
    "        dec_context = torch.sum(torch.cat(contexts,1),1) # B,1,D\n",
    "        \n",
    "        del d_tis\n",
    "        del contexts\n",
    "        \n",
    "        return dec_context\n",
    "    \n",
    "    def forward(self, input,teacher_forcing,context,encoder_outputs,encoder_maskings,training=True):\n",
    "        \"\"\"\n",
    "        input : B,1 (Start_token)\n",
    "        teacher_forcing : B,T # 수렴을 빠르게 할 목적으로 그 다음 인풋을 Ground_Truth로 알려준다\n",
    "        context : B,1,D\n",
    "        encoder_outpouts : B,T,D\n",
    "        \"\"\"\n",
    "        # Get the embedding of the current input word\n",
    "        embedded = self.embedding(input)\n",
    "        self.hidden = self.init_hidden()\n",
    "        #embedded = self.dropout(embedded)\n",
    "        \n",
    "        decode=[]\n",
    "        \n",
    "        enc_context = context\n",
    "        dec_context = Variable(torch.zeros(context.size())).cuda() if USE_CUDA else Variable(torch.zeros(context.size()))\n",
    "        decoder_outputs=[]\n",
    "        \n",
    "        \n",
    "        for i in range(self.max_len):\n",
    "\n",
    "\n",
    "            _, self.hidden = self.lstm(embedded, self.hidden)\n",
    "            decoder_outputs.append(self.hidden[0])\n",
    "            concated = torch.cat((self.hidden[0],enc_context.transpose(0,1),dec_context.transpose(0,1)),2)\n",
    "            \n",
    "            switch = torch.round(self.sigmoid(self.switch(concated.squeeze(0)))).squeeze(1) # 0 or 1 copy할지 decode할지 결정\n",
    "            score = self.out(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decode.append(softmaxed)\n",
    "            _, dec_index = torch.max(softmaxed,1)\n",
    "            \n",
    "            \n",
    "            # encoder Context & Copy Index when swith = 1\n",
    "            enc_context, copy_attn_index = self.IntraEncAttention(self.hidden[0], encoder_outputs,encoder_maskings) \n",
    "            \n",
    "            # decoder Context\n",
    "            if len(decoder_outputs)>=1:\n",
    "                dec_context = self.IntraDecAttention(self.hidden[0],torch.cat(decoder_outputs).transpose(0,1))\n",
    "            \n",
    "            \n",
    "            # teacher forcing !!\n",
    "            if training: \n",
    "                input = teacher_forcing.transpose(0,1)[i].unsqueeze(1) # T,B [i] ==> B,1\n",
    "                embedded = self.embedding(input)\n",
    "                \n",
    "            # decode or copy\n",
    "            else:\n",
    "                input=[]\n",
    "                for i,s in enumerate(switch):\n",
    "                    if s==1:\n",
    "                        input.append(copy_attn_index[i]) # copy\n",
    "                    else:\n",
    "                        input.append(dec_index[i]) # decode\n",
    "\n",
    "                embedded = self.embedding(torch.cat(input).unsqueeze(1))\n",
    "        \n",
    "        \n",
    "        # 요고 주의! time-step을 column-wise concat한 후, reshape!!\n",
    "        scores = torch.cat(decode,1)\n",
    "        \n",
    "        del decode\n",
    "        del decoder_outputs\n",
    "        \n",
    "        return scores.view(self.batch_size*self.max_len,-1), switch # 0 or 1 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "INPUT_LEN = 300\n",
    "OUTPUT_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(len(word2index),50,100,BATCH_SIZE)\n",
    "decoder = DecoderRNN(len(word2index),50,200,OUTPUT_LEN,BATCH_SIZE)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs, masks, targets = get_batch(train_data,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 300]) torch.Size([32, 300]) torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.size(), masks.size(), targets.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_output, context = encoder(inputs,masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_input = Variable(torch.LongTensor([[word2index[\"<START>\"]]*BATCH_SIZE])).cuda() if USE_CUDA else Variable(torch.LongTensor([[word2index[\"<START>\"]]*BATCH_SIZE]))\n",
    "decoder_input = decoder_input.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "decoding_scores, switch_scores = decoder(decoder_input,targets,context,enc_output,masks) # 마스킹해서 그런가 존나 느리네..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " -9.8263  -9.7942  -9.8340  ...   -9.8234  -9.8917  -9.8196\n",
       " -9.8295  -9.9243  -9.7618  ...   -9.8667  -9.8651  -9.8761\n",
       " -9.8290  -9.9148  -9.8103  ...   -9.8382  -9.8404  -9.8565\n",
       "           ...               ⋱              ...            \n",
       " -9.8607  -9.7767  -9.8090  ...   -9.8035  -9.8220  -9.8625\n",
       " -9.8613  -9.7771  -9.8092  ...   -9.8026  -9.8232  -9.8622\n",
       " -9.8618  -9.7776  -9.8093  ...   -9.8018  -9.8243  -9.8618\n",
       "[torch.FloatTensor of size 1600x18143]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoding_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실험1 :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = mask.unsqueeze(2).expand_as(output)\n",
    "output.masked_fill(mask,-1e12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. 우선 teacher forcing (MLE)부터 해서 어텐션 및 마스킹이 제대로 들어갔는지부터 검증\n",
    "2. switch score의 ground Truth를 훈련시키기 위해 NER로 만드는거 고민\n",
    "3. 디코딩 액션을 선택하기 위해 log-prob을 어떻게 만들지... (policy gradient) \n",
    "4. mixed objective 최적화 (RL+MLE)\n",
    "5. softmax 연산의 부하를 줄이기 위한 방법 (negative sampling, sampled softmax 등)\n",
    "6. 나중에 teacher forcing anealing?! 확률적으로 동작하도록... \n",
    "7. GloVe로 임베딩 매트릭스 초기화 및 고정, 인코더-디코더-프로젝션 웨이트 공유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
